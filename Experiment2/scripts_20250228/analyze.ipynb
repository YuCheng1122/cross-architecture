{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/tommy/miniconda3/envs/cross-architecture/lib/python3.12/site-packages (from scikit-learn) (2.2.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/tommy/miniconda3/envs/cross-architecture/lib/python3.12/site-packages (from scikit-learn) (1.15.1)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and processing data from /home/tommy/cross-architecture/Experiment2/csv/20250228_cleanedtrain_benign_file_features.csv and /home/tommy/cross-architecture/Experiment2/csv/20250228_cleanedtrain_malware_file_features.csv...\n",
      "Total samples: 5286\n",
      "Benign samples: 2298\n",
      "Malware samples: 2988\n",
      "CPU types: ARM, MIPS R3000, Intel 80386, Advanced Micro Devices X86-64\n",
      "Number of non-zero features: 33\n",
      "\n",
      "Classification Performance:\n",
      "Accuracy: 0.9710\n",
      "Precision: 0.9896\n",
      "Recall: 0.9588\n",
      "F1 Score: 0.9740\n",
      "5-Fold CV F1: 0.9776 (±0.0034)\n",
      "\n",
      "Classification Performance by CPU Type:\n",
      "CPU ARM: F1 Score = 0.9761 (±0.0065)\n",
      "  - Benign samples: 627\n",
      "  - Malware samples: 779\n",
      "CPU MIPS R3000: F1 Score = 0.9718 (±0.0101)\n",
      "  - Benign samples: 672\n",
      "  - Malware samples: 740\n",
      "CPU Intel 80386: F1 Score = 0.9943 (±0.0057)\n",
      "  - Benign samples: 795\n",
      "  - Malware samples: 791\n",
      "CPU Advanced Micro Devices X86-64: F1 Score = 0.9756 (±0.0040)\n",
      "  - Benign samples: 204\n",
      "  - Malware samples: 678\n",
      "\n",
      "PCA Analysis:\n",
      "Number of components needed for 80% variance: 6\n",
      "Number of components needed for 90% variance: 9\n",
      "Total number of features: 33\n",
      "\n",
      "Conclusion:\n",
      "Based on the classification metrics and feature importance analysis,\n",
      "the most important features for malware detection are: Avg_LOGICAL, Avg_INTEGER_COMPARISON, Avg_ARITHMETIC, Avg_CFG_EDGE_TO_NODE_RATIO, Avg_CFG_EDGES\n",
      "See the full analysis results in the 'results' and 'plots' directories.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "def load_and_process_data(benign_file, malware_file):\n",
    "    \"\"\"Load and process benign and malware sample data\"\"\"\n",
    "    print(f\"Loading and processing data from {benign_file} and {malware_file}...\")\n",
    "    \n",
    "    benign_df = pd.read_csv(benign_file)\n",
    "    malware_df = pd.read_csv(malware_file)\n",
    "    \n",
    "    # Create label column, 0 for benign, 1 for malware\n",
    "    benign_df['is_malware'] = 0\n",
    "    malware_df['is_malware'] = 1\n",
    "    \n",
    "    # Combine datasets\n",
    "    combined_df = pd.concat([benign_df, malware_df])\n",
    "    \n",
    "    # Select features (exclude non-numerical and zero columns)\n",
    "    exclude_cols = ['file_name', 'CPU', 'label', 'family', 'is_malware']\n",
    "    feature_cols = [col for col in combined_df.columns if col not in exclude_cols]\n",
    "    \n",
    "    # Remove zero-value features\n",
    "    non_zero_features = []\n",
    "    for col in feature_cols:\n",
    "        if combined_df[col].sum() > 0:\n",
    "            non_zero_features.append(col)\n",
    "    \n",
    "    # Print information\n",
    "    print(f\"Total samples: {len(combined_df)}\")\n",
    "    print(f\"Benign samples: {len(benign_df)}\")\n",
    "    print(f\"Malware samples: {len(malware_df)}\")\n",
    "    print(f\"CPU types: {', '.join(combined_df['CPU'].unique())}\")\n",
    "    print(f\"Number of non-zero features: {len(non_zero_features)}\")\n",
    "    \n",
    "    return combined_df, non_zero_features\n",
    "\n",
    "def evaluate_classification_performance(df, feature_cols):\n",
    "    \"\"\"Evaluate how well the features can classify malware\"\"\"\n",
    "    # Prepare data\n",
    "    X = df[feature_cols].fillna(0)\n",
    "    y = df['is_malware']\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Train Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred = rf.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(rf, X_scaled, y, cv=5, scoring='f1')\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nClassification Performance:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"5-Fold CV F1: {cv_scores.mean():.4f} (±{cv_scores.std():.4f})\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    os.makedirs(\"./plots\", exist_ok=True)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Benign', 'Malware'],\n",
    "                yticklabels=['Benign', 'Malware'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./plots/confusion_matrix.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Get feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'Importance': rf.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Plot top 15 important features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance.head(15)\n",
    "    sns.barplot(x='Importance', y='Feature', data=top_features)\n",
    "    plt.title('Top 15 Features for Malware Classification')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./plots/top_features.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    return feature_importance, rf\n",
    "\n",
    "def evaluate_by_cpu(df, feature_cols):\n",
    "    \"\"\"Evaluate classification performance for each CPU type\"\"\"\n",
    "    print(\"\\nClassification Performance by CPU Type:\")\n",
    "    \n",
    "    results = {}\n",
    "    cpu_types = df['CPU'].unique()\n",
    "    \n",
    "    for cpu in cpu_types:\n",
    "        # Skip null values\n",
    "        if pd.isna(cpu):\n",
    "            continue\n",
    "            \n",
    "        # Get subset for this CPU\n",
    "        cpu_df = df[df['CPU'] == cpu]\n",
    "        \n",
    "        # Check if we have enough samples\n",
    "        benign_count = len(cpu_df[cpu_df['is_malware'] == 0])\n",
    "        malware_count = len(cpu_df[cpu_df['is_malware'] == 1])\n",
    "        \n",
    "        if benign_count < 10 or malware_count < 10:\n",
    "            print(f\"CPU {cpu}: Insufficient samples (Benign: {benign_count}, Malware: {malware_count})\")\n",
    "            continue\n",
    "            \n",
    "        # Prepare data\n",
    "        X = cpu_df[feature_cols].fillna(0)\n",
    "        y = cpu_df['is_malware']\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # Cross-validation\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        cv_scores = cross_val_score(rf, X_scaled, y, cv=5, scoring='f1')\n",
    "        \n",
    "        print(f\"CPU {cpu}: F1 Score = {cv_scores.mean():.4f} (±{cv_scores.std():.4f})\")\n",
    "        print(f\"  - Benign samples: {benign_count}\")\n",
    "        print(f\"  - Malware samples: {malware_count}\")\n",
    "        \n",
    "        results[cpu] = {\n",
    "            'f1_mean': cv_scores.mean(),\n",
    "            'f1_std': cv_scores.std(),\n",
    "            'benign_count': benign_count,\n",
    "            'malware_count': malware_count\n",
    "        }\n",
    "        \n",
    "    return results\n",
    "\n",
    "def visualize_pca_explained_variance(df, feature_cols):\n",
    "    \"\"\"Visualize PCA explained variance to see how many features are needed\"\"\"\n",
    "    # Prepare data\n",
    "    X = df[feature_cols].fillna(0)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # PCA with all components\n",
    "    pca = PCA()\n",
    "    pca.fit(X_scaled)\n",
    "    \n",
    "    # Calculate cumulative explained variance\n",
    "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')\n",
    "    plt.axhline(y=0.8, color='r', linestyle='--', label='80% Explained Variance')\n",
    "    plt.axhline(y=0.9, color='g', linestyle='--', label='90% Explained Variance')\n",
    "    \n",
    "    # Find number of components for thresholds\n",
    "    n_components_80 = np.argmax(cumulative_variance >= 0.8) + 1\n",
    "    n_components_90 = np.argmax(cumulative_variance >= 0.9) + 1\n",
    "    \n",
    "    plt.axvline(x=n_components_80, color='r', linestyle=':', \n",
    "                label=f'{n_components_80} components for 80%')\n",
    "    plt.axvline(x=n_components_90, color='g', linestyle=':', \n",
    "                label=f'{n_components_90} components for 90%')\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title('PCA Explained Variance')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./plots/pca_explained_variance.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"\\nPCA Analysis:\")\n",
    "    print(f\"Number of components needed for 80% variance: {n_components_80}\")\n",
    "    print(f\"Number of components needed for 90% variance: {n_components_90}\")\n",
    "    print(f\"Total number of features: {len(feature_cols)}\")\n",
    "    \n",
    "    return n_components_80, n_components_90\n",
    "\n",
    "def main():\n",
    "    # File paths\n",
    "    benign_file = '/home/tommy/cross-architecture/Experiment2/csv/20250228_cleanedtrain_benign_file_features.csv'\n",
    "    malware_file = '/home/tommy/cross-architecture/Experiment2/csv/20250228_cleanedtrain_malware_file_features.csv'\n",
    "    \n",
    "    # Load and process data\n",
    "    df, feature_cols = load_and_process_data(benign_file, malware_file)\n",
    "    \n",
    "    # Evaluate classification performance\n",
    "    feature_importance, model = evaluate_classification_performance(df, feature_cols)\n",
    "    \n",
    "    # Save top features\n",
    "    os.makedirs(\"./results\", exist_ok=True)\n",
    "    feature_importance.to_csv('./results/feature_importance.csv', index=False)\n",
    "    \n",
    "    # Evaluate by CPU\n",
    "    cpu_results = evaluate_by_cpu(df, feature_cols)\n",
    "    \n",
    "    # PCA analysis\n",
    "    visualize_pca_explained_variance(df, feature_cols)\n",
    "    \n",
    "    print(\"\\nConclusion:\")\n",
    "    print(\"Based on the classification metrics and feature importance analysis,\")\n",
    "    top_features = \", \".join(feature_importance.head(5)['Feature'].tolist())\n",
    "    print(f\"the most important features for malware detection are: {top_features}\")\n",
    "    print(\"See the full analysis results in the 'results' and 'plots' directories.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Distribution by CPU Architecture:\n",
      "                               Benign  Malware\n",
      "CPU                                           \n",
      "ARM                               627      779\n",
      "Advanced Micro Devices X86-64     204      678\n",
      "Intel 80386                       795      791\n",
      "MIPS R3000                        672      740\n",
      "\n",
      "\n",
      "Key Characteristics by CPU Architecture:\n",
      "\n",
      "ARM Architecture:\n",
      "  Top Discriminative Features:\n",
      "  - Avg_CFG_EDGE_TO_NODE_RATIO: Higher in Malware, Effect Size: 1.69\n",
      "  - Avg_CFG_EDGES: Higher in Malware, Effect Size: 1.16\n",
      "  - Avg_BRANCHING: Higher in Malware, Effect Size: 1.12\n",
      "  - Avg_BOOLEAN: Higher in Malware, Effect Size: 1.11\n",
      "  - Avg_INTEGER_COMPARISON: Higher in Malware, Effect Size: 1.09\n",
      "\n",
      "Advanced Micro Devices X86-64 Architecture:\n",
      "  Top Discriminative Features:\n",
      "  - Avg_CFG_EDGE_TO_NODE_RATIO: Higher in Malware, Effect Size: 1.45\n",
      "  - Avg_CFG_NODES: Higher in Malware, Effect Size: 0.97\n",
      "  - Avg_FLOATING_POINT_COMPARE: Higher in Malware, Effect Size: 0.95\n",
      "  - Avg_CFG_EDGES: Higher in Malware, Effect Size: 0.92\n",
      "  - Avg_BOOLEAN: Higher in Malware, Effect Size: 0.89\n",
      "\n",
      "Intel 80386 Architecture:\n",
      "  Top Discriminative Features:\n",
      "  - Avg_CFG_EDGE_TO_NODE_RATIO: Higher in Malware, Effect Size: 2.53\n",
      "  - Avg_BRANCHING: Higher in Malware, Effect Size: 2.43\n",
      "  - Avg_INTEGER_COMPARISON: Higher in Malware, Effect Size: 2.24\n",
      "  - Avg_TOTAL_OPS: Higher in Malware, Effect Size: 1.79\n",
      "  - Avg_CFG_EDGES: Higher in Malware, Effect Size: 1.65\n",
      "\n",
      "MIPS R3000 Architecture:\n",
      "  Top Discriminative Features:\n",
      "  - Avg_INTEGER_COMPARISON: Higher in Malware, Effect Size: 0.97\n",
      "  - Avg_FLOATING_POINT_COMPARE: Higher in Malware, Effect Size: 0.67\n",
      "  - Avg_LOGICAL: Higher in Malware, Effect Size: 0.62\n",
      "  - Avg_DATA_MOVING: Higher in Malware, Effect Size: 0.61\n",
      "  - Total_FLOATING_POINT_COMPARE: Higher in Malware, Effect Size: 0.53\n",
      "\n",
      "Common Patterns Across Architectures:\n",
      "  - Avg_ARITHMETIC: Important in ARM, Advanced Micro Devices X86-64\n",
      "  - Avg_CFG_EDGE_TO_NODE_RATIO: Important in ARM, Advanced Micro Devices X86-64, Intel 80386, MIPS R3000\n",
      "  - Avg_CFG_NODES: Important in Advanced Micro Devices X86-64, Intel 80386\n",
      "  - Avg_INTEGER_COMPARISON: Important in ARM, Intel 80386, MIPS R3000\n",
      "  - Avg_LOGICAL: Important in ARM, Advanced Micro Devices X86-64, Intel 80386, MIPS R3000\n",
      "  - Total_LOGICAL: Important in Advanced Micro Devices X86-64, MIPS R3000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "def load_data(benign_file, malware_file):\n",
    "    \"\"\"Load benign and malware data\"\"\"\n",
    "    benign_df = pd.read_csv(benign_file)\n",
    "    malware_df = pd.read_csv(malware_file)\n",
    "    \n",
    "    # Add labels\n",
    "    benign_df['is_malware'] = 0\n",
    "    malware_df['is_malware'] = 1\n",
    "    \n",
    "    return benign_df, malware_df\n",
    "\n",
    "def analyze_cpu_distribution(benign_df, malware_df):\n",
    "    \"\"\"Analyze sample distribution by CPU architecture\"\"\"\n",
    "    # Count samples by CPU\n",
    "    benign_counts = benign_df['CPU'].value_counts()\n",
    "    malware_counts = malware_df['CPU'].value_counts()\n",
    "    \n",
    "    # Create a DataFrame for plotting\n",
    "    cpu_counts = pd.DataFrame({\n",
    "        'Benign': benign_counts,\n",
    "        'Malware': malware_counts\n",
    "    }).fillna(0)\n",
    "    \n",
    "    # Print distribution\n",
    "    print(\"Sample Distribution by CPU Architecture:\")\n",
    "    print(cpu_counts)\n",
    "    print()\n",
    "    \n",
    "    # Plot distribution\n",
    "    os.makedirs('./plots', exist_ok=True)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    cpu_counts.plot(kind='bar')\n",
    "    plt.title('Sample Distribution by CPU Architecture')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.xlabel('CPU Architecture')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./plots/cpu_distribution.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    return cpu_counts\n",
    "\n",
    "def get_top_features_by_cpu(benign_df, malware_df, cpu_type):\n",
    "    \"\"\"Get top discriminative features for a specific CPU architecture\"\"\"\n",
    "    # Filter by CPU\n",
    "    benign_subset = benign_df[benign_df['CPU'] == cpu_type]\n",
    "    malware_subset = malware_df[malware_df['CPU'] == cpu_type]\n",
    "    \n",
    "    # Skip if not enough samples\n",
    "    if len(benign_subset) < 5 or len(malware_subset) < 5:\n",
    "        print(f\"Not enough samples for {cpu_type}\")\n",
    "        return None\n",
    "    \n",
    "    # Select features (exclude non-numeric columns)\n",
    "    exclude_cols = ['file_name', 'CPU', 'label', 'family', 'is_malware']\n",
    "    feature_cols = [col for col in benign_subset.columns if col not in exclude_cols]\n",
    "    \n",
    "    # Filter out zero-value features\n",
    "    non_zero_features = []\n",
    "    combined = pd.concat([benign_subset, malware_subset])\n",
    "    for col in feature_cols:\n",
    "        if combined[col].sum() > 0:\n",
    "            non_zero_features.append(col)\n",
    "    \n",
    "    # Calculate statistics for each feature\n",
    "    feature_stats = []\n",
    "    for feature in non_zero_features:\n",
    "        benign_mean = benign_subset[feature].mean()\n",
    "        malware_mean = malware_subset[feature].mean()\n",
    "        \n",
    "        # Calculate absolute difference and percent difference\n",
    "        abs_diff = abs(benign_mean - malware_mean)\n",
    "        if malware_mean != 0:\n",
    "            pct_diff = abs_diff / malware_mean * 100\n",
    "        else:\n",
    "            pct_diff = float('inf') if abs_diff > 0 else 0\n",
    "            \n",
    "        # Calculate effect size (Cohen's d)\n",
    "        pooled_std = np.sqrt((benign_subset[feature].std()**2 + malware_subset[feature].std()**2) / 2)\n",
    "        effect_size = abs_diff / pooled_std if pooled_std > 0 else 0\n",
    "        \n",
    "        feature_stats.append({\n",
    "            'Feature': feature,\n",
    "            'Benign Mean': benign_mean,\n",
    "            'Malware Mean': malware_mean,\n",
    "            'Absolute Difference': abs_diff,\n",
    "            'Percent Difference': pct_diff,\n",
    "            'Effect Size': effect_size\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame and sort by effect size\n",
    "    stats_df = pd.DataFrame(feature_stats).sort_values('Effect Size', ascending=False)\n",
    "    \n",
    "    return stats_df\n",
    "\n",
    "def train_classifier_by_cpu(benign_df, malware_df, cpu_type):\n",
    "    \"\"\"Train a Random Forest classifier for a specific CPU and get feature importance\"\"\"\n",
    "    # Filter by CPU\n",
    "    benign_subset = benign_df[benign_df['CPU'] == cpu_type]\n",
    "    malware_subset = malware_df[malware_df['CPU'] == cpu_type]\n",
    "    \n",
    "    # Skip if not enough samples\n",
    "    if len(benign_subset) < 5 or len(malware_subset) < 5:\n",
    "        return None\n",
    "    \n",
    "    # Combine datasets\n",
    "    combined = pd.concat([benign_subset, malware_subset])\n",
    "    \n",
    "    # Select features\n",
    "    exclude_cols = ['file_name', 'CPU', 'label', 'family', 'is_malware']\n",
    "    feature_cols = [col for col in combined.columns if col not in exclude_cols and combined[col].sum() > 0]\n",
    "    \n",
    "    # Prepare data\n",
    "    X = combined[feature_cols].fillna(0)\n",
    "    y = combined['is_malware']\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Train Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_scaled, y)\n",
    "    \n",
    "    # Get feature importance\n",
    "    importance = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'Importance': rf.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    return importance\n",
    "\n",
    "def plot_top_features_comparison(benign_df, malware_df, cpu_types, top_n=10):\n",
    "    \"\"\"Plot top feature means comparison for each CPU architecture\"\"\"\n",
    "    os.makedirs('./plots', exist_ok=True)\n",
    "    os.makedirs('./results', exist_ok=True)\n",
    "    \n",
    "    for cpu_type in cpu_types:\n",
    "        # Get top features\n",
    "        stats_df = get_top_features_by_cpu(benign_df, malware_df, cpu_type)\n",
    "        \n",
    "        if stats_df is None or len(stats_df) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Get top features by effect size\n",
    "        top_features = stats_df.head(top_n)\n",
    "        \n",
    "        # Save results\n",
    "        stats_df.to_csv(f'./results/feature_stats_{cpu_type}.csv', index=False)\n",
    "        \n",
    "        # Create data for plotting\n",
    "        plot_data = []\n",
    "        for _, row in top_features.iterrows():\n",
    "            plot_data.append({\n",
    "                'Feature': row['Feature'],\n",
    "                'Type': 'Benign',\n",
    "                'Value': row['Benign Mean']\n",
    "            })\n",
    "            plot_data.append({\n",
    "                'Feature': row['Feature'],\n",
    "                'Type': 'Malware',\n",
    "                'Value': row['Malware Mean']\n",
    "            })\n",
    "        \n",
    "        plot_df = pd.DataFrame(plot_data)\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        chart = sns.barplot(x='Value', y='Feature', hue='Type', data=plot_df)\n",
    "        plt.title(f'Top {top_n} Discriminative Features for {cpu_type}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'./plots/top_features_{cpu_type}.png', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Plot feature importance from Random Forest\n",
    "        importance = train_classifier_by_cpu(benign_df, malware_df, cpu_type)\n",
    "        \n",
    "        if importance is not None and len(importance) > 0:\n",
    "            importance.to_csv(f'./results/feature_importance_{cpu_type}.csv', index=False)\n",
    "            \n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.barplot(x='Importance', y='Feature', data=importance.head(top_n))\n",
    "            plt.title(f'Top {top_n} Important Features for {cpu_type} Classification')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'./plots/feature_importance_{cpu_type}.png', dpi=300)\n",
    "            plt.close()\n",
    "\n",
    "def compare_features_across_architectures(benign_df, malware_df, cpu_types, top_n=5):\n",
    "    \"\"\"Compare the top features across different CPU architectures\"\"\"\n",
    "    # Get top features for each architecture\n",
    "    all_top_features = {}\n",
    "    \n",
    "    for cpu_type in cpu_types:\n",
    "        importance = train_classifier_by_cpu(benign_df, malware_df, cpu_type)\n",
    "        if importance is not None and len(importance) > 0:\n",
    "            all_top_features[cpu_type] = importance.head(top_n)['Feature'].tolist()\n",
    "    \n",
    "    # Find common features\n",
    "    all_features = []\n",
    "    for cpu, features in all_top_features.items():\n",
    "        all_features.extend(features)\n",
    "    \n",
    "    unique_features = sorted(list(set(all_features)))\n",
    "    \n",
    "    # Create comparison matrix\n",
    "    comparison = pd.DataFrame(0, index=unique_features, columns=cpu_types)\n",
    "    \n",
    "    for cpu, features in all_top_features.items():\n",
    "        for i, feature in enumerate(features):\n",
    "            # Use reverse rank as value (top feature gets highest score)\n",
    "            comparison.loc[feature, cpu] = top_n - i\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(10, len(unique_features) * 0.4 + 2))\n",
    "    sns.heatmap(comparison, cmap='YlGnBu', linewidths=0.5, annot=True, fmt='.0f')\n",
    "    plt.title('Feature Importance Across CPU Architectures')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./plots/feature_comparison_across_cpus.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Save comparison\n",
    "    comparison.to_csv('./results/feature_comparison_across_cpus.csv')\n",
    "    \n",
    "    return comparison\n",
    "\n",
    "def main():\n",
    "    # File paths\n",
    "    benign_file = '/home/tommy/cross-architecture/Experiment2/csv/20250228_cleanedtrain_benign_file_features.csv'\n",
    "    malware_file = '/home/tommy/cross-architecture/Experiment2/csv/20250228_cleanedtrain_malware_file_features.csv'\n",
    "    \n",
    "    # Load data\n",
    "    benign_df, malware_df = load_data(benign_file, malware_file)\n",
    "    \n",
    "    # Get CPU types\n",
    "    cpu_types = sorted(list(set(benign_df['CPU'].unique()) | set(malware_df['CPU'].unique())))\n",
    "    cpu_types = [cpu for cpu in cpu_types if not pd.isna(cpu)]\n",
    "    \n",
    "    # Analyze CPU distribution\n",
    "    analyze_cpu_distribution(benign_df, malware_df)\n",
    "    \n",
    "    # Plot top features for each CPU\n",
    "    plot_top_features_comparison(benign_df, malware_df, cpu_types)\n",
    "    \n",
    "    # Compare features across architectures\n",
    "    compare_features_across_architectures(benign_df, malware_df, cpu_types)\n",
    "    \n",
    "    # Print conclusions for each architecture\n",
    "    print(\"\\nKey Characteristics by CPU Architecture:\")\n",
    "    for cpu_type in cpu_types:\n",
    "        print(f\"\\n{cpu_type} Architecture:\")\n",
    "        \n",
    "        stats_df = get_top_features_by_cpu(benign_df, malware_df, cpu_type)\n",
    "        if stats_df is None or len(stats_df) == 0:\n",
    "            print(f\"  Insufficient samples for analysis\")\n",
    "            continue\n",
    "            \n",
    "        # Get top 5 features with largest effect size\n",
    "        top_features = stats_df.head(5)\n",
    "        \n",
    "        print(f\"  Top Discriminative Features:\")\n",
    "        for _, row in top_features.iterrows():\n",
    "            direction = \"Higher in Benign\" if row['Benign Mean'] > row['Malware Mean'] else \"Higher in Malware\"\n",
    "            print(f\"  - {row['Feature']}: {direction}, Effect Size: {row['Effect Size']:.2f}\")\n",
    "    \n",
    "    print(\"\\nCommon Patterns Across Architectures:\")\n",
    "    # Look for features that appear in top 5 for multiple architectures\n",
    "    importance_comparison = compare_features_across_architectures(benign_df, malware_df, cpu_types)\n",
    "    \n",
    "    # Features that appear in at least 2 architectures\n",
    "    common_features = importance_comparison[(importance_comparison > 0).sum(axis=1) >= 2]\n",
    "    \n",
    "    if len(common_features) > 0:\n",
    "        for feature in common_features.index:\n",
    "            architectures = [cpu for cpu in cpu_types if importance_comparison.loc[feature, cpu] > 0]\n",
    "            print(f\"  - {feature}: Important in {', '.join(architectures)}\")\n",
    "    else:\n",
    "        print(\"  No common important features found across architectures\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Sample distribution by CPU architecture:\n",
      "                               Benign  Malware\n",
      "CPU                                           \n",
      "ARM                               627      779\n",
      "Advanced Micro Devices X86-64     204      678\n",
      "Intel 80386                       795      791\n",
      "MIPS R3000                        672      740\n",
      "Using 33 non-zero features\n",
      "Training on ARM...\n",
      "  Testing on ARM...\n",
      "    Accuracy: 1.0000, F1: 1.0000\n",
      "  Testing on Advanced Micro Devices X86-64...\n",
      "    Accuracy: 0.5771, F1: 0.6205\n",
      "  Testing on Intel 80386...\n",
      "    Accuracy: 0.8348, F1: 0.8015\n",
      "  Testing on MIPS R3000...\n",
      "    Accuracy: 0.6629, F1: 0.5441\n",
      "Training on Advanced Micro Devices X86-64...\n",
      "  Testing on ARM...\n",
      "    Accuracy: 0.8634, F1: 0.8889\n",
      "  Testing on Advanced Micro Devices X86-64...\n",
      "    Accuracy: 1.0000, F1: 1.0000\n",
      "  Testing on Intel 80386...\n",
      "    Accuracy: 0.9817, F1: 0.9817\n",
      "  Testing on MIPS R3000...\n",
      "    Accuracy: 0.8088, F1: 0.8391\n",
      "Training on Intel 80386...\n",
      "  Testing on ARM...\n",
      "    Accuracy: 0.8037, F1: 0.8388\n",
      "  Testing on Advanced Micro Devices X86-64...\n",
      "    Accuracy: 0.9138, F1: 0.9442\n",
      "  Testing on Intel 80386...\n",
      "    Accuracy: 1.0000, F1: 1.0000\n",
      "  Testing on MIPS R3000...\n",
      "    Accuracy: 0.6523, F1: 0.7473\n",
      "Training on MIPS R3000...\n",
      "  Testing on ARM...\n",
      "    Accuracy: 0.8620, F1: 0.8660\n",
      "  Testing on Advanced Micro Devices X86-64...\n",
      "    Accuracy: 0.8900, F1: 0.9244\n",
      "  Testing on Intel 80386...\n",
      "    Accuracy: 0.9685, F1: 0.9674\n",
      "  Testing on MIPS R3000...\n",
      "    Accuracy: 1.0000, F1: 1.0000\n",
      "Cross-architecture experiment completed. Results saved to the 'results' directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import os\n",
    "\n",
    "def load_data(benign_file, malware_file):\n",
    "    \"\"\"Load benign and malware data\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    benign_df = pd.read_csv(benign_file)\n",
    "    malware_df = pd.read_csv(malware_file)\n",
    "    \n",
    "    # Add labels\n",
    "    benign_df['is_malware'] = 0\n",
    "    malware_df['is_malware'] = 1\n",
    "    \n",
    "    return benign_df, malware_df\n",
    "\n",
    "def get_features(df):\n",
    "    \"\"\"Get feature columns, excluding non-feature columns and zero columns\"\"\"\n",
    "    exclude_cols = ['file_name', 'CPU', 'label', 'family', 'is_malware']\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    # Filter out zero columns\n",
    "    non_zero_features = []\n",
    "    for col in feature_cols:\n",
    "        if df[col].sum() > 0:\n",
    "            non_zero_features.append(col)\n",
    "    \n",
    "    return non_zero_features\n",
    "\n",
    "def cross_architecture_evaluation(benign_df, malware_df):\n",
    "    \"\"\"Perform cross-architecture experiments: train on one CPU, test on another\"\"\"\n",
    "    # Get all CPU types\n",
    "    all_cpus = sorted(list(set(benign_df['CPU'].unique()) | set(malware_df['CPU'].unique())))\n",
    "    all_cpus = [cpu for cpu in all_cpus if not pd.isna(cpu)]\n",
    "    \n",
    "    # Combine datasets\n",
    "    combined_df = pd.concat([benign_df, malware_df])\n",
    "    \n",
    "    # Get features\n",
    "    feature_cols = get_features(combined_df)\n",
    "    print(f\"Using {len(feature_cols)} non-zero features\")\n",
    "    \n",
    "    # Initialize results matrix\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    results = {}\n",
    "    for metric in metrics:\n",
    "        results[metric] = pd.DataFrame(np.zeros((len(all_cpus), len(all_cpus))), \n",
    "                                       index=all_cpus, columns=all_cpus)\n",
    "    \n",
    "    # For each CPU pair, train on one and test on another\n",
    "    for train_cpu in all_cpus:\n",
    "        print(f\"Training on {train_cpu}...\")\n",
    "        \n",
    "        # Get training data for this CPU\n",
    "        train_benign = benign_df[benign_df['CPU'] == train_cpu]\n",
    "        train_malware = malware_df[malware_df['CPU'] == train_cpu]\n",
    "        \n",
    "        # Skip if not enough samples\n",
    "        if len(train_benign) < 10 or len(train_malware) < 10:\n",
    "            print(f\"  Skipping {train_cpu} (insufficient samples)\")\n",
    "            continue\n",
    "        \n",
    "        # Combine training data\n",
    "        train_data = pd.concat([train_benign, train_malware])\n",
    "        \n",
    "        # Prepare training data\n",
    "        X_train = train_data[feature_cols].fillna(0)\n",
    "        y_train = train_data['is_malware']\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        \n",
    "        # Train Random Forest\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        rf.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Test on each CPU\n",
    "        for test_cpu in all_cpus:\n",
    "            print(f\"  Testing on {test_cpu}...\")\n",
    "            \n",
    "            # Get test data for this CPU\n",
    "            test_benign = benign_df[benign_df['CPU'] == test_cpu]\n",
    "            test_malware = malware_df[malware_df['CPU'] == test_cpu]\n",
    "            \n",
    "            # Skip if not enough samples\n",
    "            if len(test_benign) < 10 or len(test_malware) < 10:\n",
    "                print(f\"    Skipping {test_cpu} (insufficient samples)\")\n",
    "                continue\n",
    "            \n",
    "            # Combine test data\n",
    "            test_data = pd.concat([test_benign, test_malware])\n",
    "            \n",
    "            # Prepare test data\n",
    "            X_test = test_data[feature_cols].fillna(0)\n",
    "            y_test = test_data['is_malware']\n",
    "            \n",
    "            # Scale test data using training scaler\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = rf.predict(X_test_scaled)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred)\n",
    "            recall = recall_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            \n",
    "            # Store results\n",
    "            results['Accuracy'].loc[train_cpu, test_cpu] = accuracy\n",
    "            results['Precision'].loc[train_cpu, test_cpu] = precision\n",
    "            results['Recall'].loc[train_cpu, test_cpu] = recall\n",
    "            results['F1 Score'].loc[train_cpu, test_cpu] = f1\n",
    "            \n",
    "            print(f\"    Accuracy: {accuracy:.4f}, F1: {f1:.4f}\")\n",
    "    \n",
    "    return results, all_cpus\n",
    "\n",
    "def plot_results(results, all_cpus, output_dir='./results'):\n",
    "    \"\"\"Plot result matrices\"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Plot each metric\n",
    "    for metric, matrix in results.items():\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(matrix, annot=True, fmt='.4f', cmap='YlGnBu', vmin=0, vmax=1)\n",
    "        plt.title(f'Cross-Architecture {metric}')\n",
    "        plt.xlabel('Test CPU')\n",
    "        plt.ylabel('Train CPU')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/{metric.lower().replace(\" \", \"_\")}_matrix.png', dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    # Save matrices to CSV\n",
    "    for metric, matrix in results.items():\n",
    "        matrix.to_csv(f'{output_dir}/{metric.lower().replace(\" \", \"_\")}_matrix.csv')\n",
    "\n",
    "def main():\n",
    "    # File paths\n",
    "    benign_file = '/home/tommy/cross-architecture/Experiment2/csv/20250228_cleanedtrain_benign_file_features.csv'\n",
    "    malware_file = '/home/tommy/cross-architecture/Experiment2/csv/20250228_cleanedtrain_malware_file_features.csv'\n",
    "    \n",
    "    # Load data\n",
    "    benign_df, malware_df = load_data(benign_file, malware_file)\n",
    "    \n",
    "    # Get CPU distribution\n",
    "    benign_counts = benign_df['CPU'].value_counts()\n",
    "    malware_counts = malware_df['CPU'].value_counts()\n",
    "    \n",
    "    print(\"Sample distribution by CPU architecture:\")\n",
    "    print(pd.DataFrame({\n",
    "        'Benign': benign_counts,\n",
    "        'Malware': malware_counts\n",
    "    }).fillna(0))\n",
    "    \n",
    "    # Run cross-architecture evaluation\n",
    "    results, all_cpus = cross_architecture_evaluation(benign_df, malware_df)\n",
    "    \n",
    "    # Plot results\n",
    "    plot_results(results, all_cpus)\n",
    "    \n",
    "    print(\"Cross-architecture experiment completed. Results saved to the 'results' directory.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Training data CPU distribution:\n",
      "                               Benign  Malware\n",
      "CPU                                           \n",
      "ARM                               627      779\n",
      "Advanced Micro Devices X86-64     204      678\n",
      "Intel 80386                       795      791\n",
      "MIPS R3000                        672      740\n",
      "\n",
      "Testing data CPU distribution:\n",
      "         Benign  Malware\n",
      "CPU                     \n",
      "PowerPC     794      794\n",
      "\n",
      "Using 33 non-zero features\n",
      "Training on ARM, testing on PowerPC...\n",
      "  Training data: 627 benign, 779 malware\n",
      "  Testing data: 794 benign, 794 malware\n",
      "  F1 Score: 0.5849\n",
      "Training on Advanced Micro Devices X86-64, testing on PowerPC...\n",
      "  Training data: 204 benign, 678 malware\n",
      "  Testing data: 794 benign, 794 malware\n",
      "  F1 Score: 0.7401\n",
      "Training on Intel 80386, testing on PowerPC...\n",
      "  Training data: 795 benign, 791 malware\n",
      "  Testing data: 794 benign, 794 malware\n",
      "  F1 Score: 0.7460\n",
      "Training on MIPS R3000, testing on PowerPC...\n",
      "  Training data: 672 benign, 740 malware\n",
      "  Testing data: 794 benign, 794 malware\n",
      "  F1 Score: 0.9361\n",
      "Training on all, testing on PowerPC...\n",
      "  Training data: 2298 benign, 2988 malware\n",
      "  Testing data: 794 benign, 794 malware\n",
      "  F1 Score: 0.9535\n",
      "\n",
      "Experiment completed. Results saved to the 'results' directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "import os\n",
    "\n",
    "def load_data(train_benign_file, train_malware_file, test_benign_file, test_malware_file):\n",
    "    \"\"\"Load training and testing data\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    \n",
    "    # Load training data\n",
    "    train_benign_df = pd.read_csv(train_benign_file)\n",
    "    train_malware_df = pd.read_csv(train_malware_file)\n",
    "    \n",
    "    # Load testing data\n",
    "    test_benign_df = pd.read_csv(test_benign_file)\n",
    "    test_malware_df = pd.read_csv(test_malware_file)\n",
    "    \n",
    "    # Add labels\n",
    "    train_benign_df['is_malware'] = 0\n",
    "    train_malware_df['is_malware'] = 1\n",
    "    test_benign_df['is_malware'] = 0\n",
    "    test_malware_df['is_malware'] = 1\n",
    "    \n",
    "    return train_benign_df, train_malware_df, test_benign_df, test_malware_df\n",
    "\n",
    "def get_features(df):\n",
    "    \"\"\"Get feature columns, excluding non-feature columns and zero columns\"\"\"\n",
    "    exclude_cols = ['file_name', 'CPU', 'label', 'family', 'is_malware']\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    # Filter out zero columns\n",
    "    non_zero_features = []\n",
    "    for col in feature_cols:\n",
    "        if df[col].sum() > 0:\n",
    "            non_zero_features.append(col)\n",
    "    \n",
    "    return non_zero_features\n",
    "\n",
    "def evaluate_on_powerpc(train_cpu, train_benign_df, train_malware_df, \n",
    "                        test_benign_df, test_malware_df, feature_cols):\n",
    "    \"\"\"Train on specified architecture and test on PowerPC\"\"\"\n",
    "    print(f\"Training on {train_cpu}, testing on PowerPC...\")\n",
    "    \n",
    "    # Get training data\n",
    "    if train_cpu == 'all':\n",
    "        # Use all training data\n",
    "        train_benign = train_benign_df\n",
    "        train_malware = train_malware_df\n",
    "    else:\n",
    "        # Filter by CPU\n",
    "        train_benign = train_benign_df[train_benign_df['CPU'] == train_cpu]\n",
    "        train_malware = train_malware_df[train_malware_df['CPU'] == train_cpu]\n",
    "    \n",
    "    # Skip if not enough training samples\n",
    "    if len(train_benign) < 10 or len(train_malware) < 10:\n",
    "        print(f\"  Skipping {train_cpu} (insufficient training samples)\")\n",
    "        return None\n",
    "    \n",
    "    # Get testing data for PowerPC\n",
    "    test_benign = test_benign_df[test_benign_df['CPU'] == 'PowerPC']\n",
    "    test_malware = test_malware_df[test_malware_df['CPU'] == 'PowerPC']\n",
    "    \n",
    "    # Skip if not enough testing samples\n",
    "    if len(test_benign) < 5 or len(test_malware) < 5:\n",
    "        print(f\"  Skipping PowerPC (insufficient testing samples)\")\n",
    "        return None\n",
    "    \n",
    "    # Print data sizes\n",
    "    print(f\"  Training data: {len(train_benign)} benign, {len(train_malware)} malware\")\n",
    "    print(f\"  Testing data: {len(test_benign)} benign, {len(test_malware)} malware\")\n",
    "    \n",
    "    # Combine training data\n",
    "    train_data = pd.concat([train_benign, train_malware])\n",
    "    \n",
    "    # Combine testing data\n",
    "    test_data = pd.concat([test_benign, test_malware])\n",
    "    \n",
    "    # Prepare training data\n",
    "    X_train = train_data[feature_cols].fillna(0)\n",
    "    y_train = train_data['is_malware']\n",
    "    \n",
    "    # Prepare testing data\n",
    "    X_test = test_data[feature_cols].fillna(0)\n",
    "    y_test = test_data['is_malware']\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = rf.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"  F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    return f1\n",
    "\n",
    "def plot_f1_comparison(results, output_dir='./results'):\n",
    "    \"\"\"Plot and save F1 score comparison\"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get training CPUs and F1 scores\n",
    "    train_cpus = list(results.keys())\n",
    "    f1_scores = list(results.values())\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Architecture': train_cpus,\n",
    "        'F1 Score': f1_scores\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(f'{output_dir}/powerpc_f1_comparison.csv', index=False)\n",
    "    \n",
    "    # Sort by F1 score\n",
    "    df = df.sort_values('F1 Score', ascending=False)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    # Bar plot\n",
    "    bars = plt.bar(df['Architecture'], df['F1 Score'], color='steelblue')\n",
    "    \n",
    "    # Add values on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                 f'{height:.4f}', ha='center', va='bottom', fontsize=12)\n",
    "    \n",
    "    plt.title('F1 Scores on PowerPC by Training Architecture', fontsize=16)\n",
    "    plt.xlabel('Training Architecture', fontsize=14)\n",
    "    plt.ylabel('F1 Score', fontsize=14)\n",
    "    plt.ylim(0, 1.1)  # Set y-axis limit with some margin for the text\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(f'{output_dir}/powerpc_f1_comparison.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    # File paths\n",
    "    train_benign_file = '/home/tommy/cross-architecture/Experiment2/csv/20250228_cleanedtrain_benign_file_features.csv'\n",
    "    train_malware_file = '/home/tommy/cross-architecture/Experiment2/csv/20250228_cleanedtrain_malware_file_features.csv'\n",
    "    test_benign_file = '/home/tommy/cross-architecture/Experiment2/csv/20250228_cleanedtest_benign_file_features.csv'\n",
    "    test_malware_file = '/home/tommy/cross-architecture/Experiment2/csv/20250228_cleanedtest_malware_file_features.csv'\n",
    "    \n",
    "    # Load data\n",
    "    train_benign_df, train_malware_df, test_benign_df, test_malware_df = load_data(\n",
    "        train_benign_file, train_malware_file, test_benign_file, test_malware_file\n",
    "    )\n",
    "    \n",
    "    # Print CPU distributions\n",
    "    print(\"\\nTraining data CPU distribution:\")\n",
    "    train_cpu_dist = pd.DataFrame({\n",
    "        'Benign': train_benign_df['CPU'].value_counts(),\n",
    "        'Malware': train_malware_df['CPU'].value_counts()\n",
    "    }).fillna(0)\n",
    "    print(train_cpu_dist)\n",
    "    \n",
    "    print(\"\\nTesting data CPU distribution:\")\n",
    "    test_cpu_dist = pd.DataFrame({\n",
    "        'Benign': test_benign_df['CPU'].value_counts(),\n",
    "        'Malware': test_malware_df['CPU'].value_counts()\n",
    "    }).fillna(0)\n",
    "    print(test_cpu_dist)\n",
    "    \n",
    "    # Get features from all data\n",
    "    all_data = pd.concat([train_benign_df, train_malware_df, test_benign_df, test_malware_df])\n",
    "    feature_cols = get_features(all_data)\n",
    "    print(f\"\\nUsing {len(feature_cols)} non-zero features\")\n",
    "    \n",
    "    # Define CPU architectures for training\n",
    "    # Get all available architectures and add 'all'\n",
    "    available_cpus = sorted(list(set(train_benign_df['CPU'].unique()) | set(train_malware_df['CPU'].unique())))\n",
    "    available_cpus = [cpu for cpu in available_cpus if not pd.isna(cpu)]\n",
    "    train_cpus = available_cpus + ['all']\n",
    "    \n",
    "    # Evaluate each architecture on PowerPC\n",
    "    results = {}\n",
    "    for train_cpu in train_cpus:\n",
    "        f1 = evaluate_on_powerpc(\n",
    "            train_cpu,\n",
    "            train_benign_df, train_malware_df,\n",
    "            test_benign_df, test_malware_df,\n",
    "            feature_cols\n",
    "        )\n",
    "        if f1 is not None:\n",
    "            results[train_cpu] = f1\n",
    "    \n",
    "    # Plot and save results\n",
    "    if results:\n",
    "        plot_f1_comparison(results)\n",
    "    else:\n",
    "        print(\"No results to plot.\")\n",
    "    \n",
    "    print(\"\\nExperiment completed. Results saved to the 'results' directory.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Training data CPU distribution:\n",
      "                               Benign  Malware\n",
      "CPU                                           \n",
      "ARM                               627      779\n",
      "Advanced Micro Devices X86-64     204      678\n",
      "Intel 80386                       795      791\n",
      "MIPS R3000                        672      740\n",
      "\n",
      "Testing data CPU distribution:\n",
      "         Benign  Malware\n",
      "CPU                     \n",
      "PowerPC     794      794\n",
      "\n",
      "Using 33 non-zero features\n",
      "Training on ARM, testing on PowerPC...\n",
      "  Training data: 627 benign, 779 malware\n",
      "  Testing data: 794 benign, 794 malware\n",
      "  Running with seed 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Running with seed 123\n",
      "  Running with seed 456\n",
      "  Running with seed 789\n",
      "  Running with seed 101\n",
      "  Performing 5-fold cross-validation\n",
      "  Mean F1 Score: 0.5682 ± 0.0108\n",
      "  95% Confidence Interval: [0.5548, 0.5816]\n",
      "  Cross-Validation F1: 0.9806 ± 0.0042\n",
      "Training on Advanced Micro Devices X86-64, testing on PowerPC...\n",
      "  Training data: 204 benign, 678 malware\n",
      "  Testing data: 794 benign, 794 malware\n",
      "  Running with seed 42\n",
      "  Running with seed 123\n",
      "  Running with seed 456\n",
      "  Running with seed 789\n",
      "  Running with seed 101\n",
      "  Performing 5-fold cross-validation\n",
      "  Mean F1 Score: 0.7456 ± 0.0050\n",
      "  95% Confidence Interval: [0.7393, 0.7518]\n",
      "  Cross-Validation F1: 0.9734 ± 0.0064\n",
      "Training on Intel 80386, testing on PowerPC...\n",
      "  Training data: 795 benign, 791 malware\n",
      "  Testing data: 794 benign, 794 malware\n",
      "  Running with seed 42\n",
      "  Running with seed 123\n",
      "  Running with seed 456\n",
      "  Running with seed 789\n",
      "  Running with seed 101\n",
      "  Performing 5-fold cross-validation\n",
      "  Mean F1 Score: 0.7474 ± 0.0062\n",
      "  95% Confidence Interval: [0.7397, 0.7551]\n",
      "  Cross-Validation F1: 0.9937 ± 0.0020\n",
      "Training on MIPS R3000, testing on PowerPC...\n",
      "  Training data: 672 benign, 740 malware\n",
      "  Testing data: 794 benign, 794 malware\n",
      "  Running with seed 42\n",
      "  Running with seed 123\n",
      "  Running with seed 456\n",
      "  Running with seed 789\n",
      "  Running with seed 101\n",
      "  Performing 5-fold cross-validation\n",
      "  Mean F1 Score: 0.9420 ± 0.0046\n",
      "  95% Confidence Interval: [0.9362, 0.9477]\n",
      "  Cross-Validation F1: 0.9704 ± 0.0114\n",
      "Training on all, testing on PowerPC...\n",
      "  Training data: 2298 benign, 2988 malware\n",
      "  Testing data: 794 benign, 794 malware\n",
      "  Running with seed 42\n",
      "  Running with seed 123\n",
      "  Running with seed 456\n",
      "  Running with seed 789\n",
      "  Running with seed 101\n",
      "  Performing 5-fold cross-validation\n",
      "  Mean F1 Score: 0.9532 ± 0.0043\n",
      "  95% Confidence Interval: [0.9479, 0.9586]\n",
      "  Cross-Validation F1: 0.9781 ± 0.0050\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'bright_colors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 453\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExperiment completed. Results saved to the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 453\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 446\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# Plot and save results\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m all_results:\n\u001b[0;32m--> 446\u001b[0m     \u001b[43mplot_f1_comparison\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo results to plot.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 249\u001b[0m, in \u001b[0;36mplot_f1_comparison\u001b[0;34m(all_results, output_dir)\u001b[0m\n\u001b[1;32m    246\u001b[0m custom_colors \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#2D004A\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#304887\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#3D7D7C\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#4CAA66\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#B9CF45\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Make sure we have enough colors\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mbright_colors\u001b[49m) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(df):\n\u001b[1;32m    250\u001b[0m     bright_colors\u001b[38;5;241m.\u001b[39mextend(bright_colors)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# --------- Main F1 Score Chart ---------\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# Set the style for the plot\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bright_colors' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "from scipy import stats\n",
    "\n",
    "def load_data(train_benign_file, train_malware_file, test_benign_file, test_malware_file):\n",
    "    \"\"\"Load training and testing data\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    \n",
    "    # Load training data\n",
    "    train_benign_df = pd.read_csv(train_benign_file)\n",
    "    train_malware_df = pd.read_csv(train_malware_file)\n",
    "    \n",
    "    # Load testing data\n",
    "    test_benign_df = pd.read_csv(test_benign_file)\n",
    "    test_malware_df = pd.read_csv(test_malware_file)\n",
    "    \n",
    "    # Add labels\n",
    "    train_benign_df['is_malware'] = 0\n",
    "    train_malware_df['is_malware'] = 1\n",
    "    test_benign_df['is_malware'] = 0\n",
    "    test_malware_df['is_malware'] = 1\n",
    "    \n",
    "    return train_benign_df, train_malware_df, test_benign_df, test_malware_df\n",
    "\n",
    "def get_features(df):\n",
    "    \"\"\"Get feature columns, excluding non-feature columns and zero columns\"\"\"\n",
    "    exclude_cols = ['file_name', 'CPU', 'label', 'family', 'is_malware']\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    # Filter out zero columns\n",
    "    non_zero_features = []\n",
    "    for col in feature_cols:\n",
    "        if df[col].sum() > 0:\n",
    "            non_zero_features.append(col)\n",
    "    \n",
    "    return non_zero_features\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_scores=None):\n",
    "    \"\"\"Calculate multiple performance metrics\"\"\"\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred),\n",
    "        'recall': recall_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred)\n",
    "    }\n",
    "    \n",
    "    # Add ROC AUC if probabilities are available\n",
    "    if y_scores is not None:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_true, y_scores)\n",
    "    \n",
    "    # Add confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    metrics['true_negatives'] = int(tn)\n",
    "    metrics['false_positives'] = int(fp)\n",
    "    metrics['false_negatives'] = int(fn)\n",
    "    metrics['true_positives'] = int(tp)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def evaluate_on_powerpc(train_cpu, train_benign_df, train_malware_df, \n",
    "                        test_benign_df, test_malware_df, feature_cols, \n",
    "                        random_seeds=[42, 123, 456, 789, 101], cv_folds=5):\n",
    "    \"\"\"Train on specified architecture and test on PowerPC with multiple runs and CV\"\"\"\n",
    "    print(f\"Training on {train_cpu}, testing on PowerPC...\")\n",
    "    \n",
    "    # Get training data\n",
    "    if train_cpu == 'all':\n",
    "        # Use all training data\n",
    "        train_benign = train_benign_df\n",
    "        train_malware = train_malware_df\n",
    "    else:\n",
    "        # Filter by CPU\n",
    "        train_benign = train_benign_df[train_benign_df['CPU'] == train_cpu]\n",
    "        train_malware = train_malware_df[train_malware_df['CPU'] == train_cpu]\n",
    "    \n",
    "    # Skip if not enough training samples\n",
    "    if len(train_benign) < 10 or len(train_malware) < 10:\n",
    "        print(f\"  Skipping {train_cpu} (insufficient training samples)\")\n",
    "        return None\n",
    "    \n",
    "    # Get testing data for PowerPC\n",
    "    test_benign = test_benign_df[test_benign_df['CPU'] == 'PowerPC']\n",
    "    test_malware = test_malware_df[test_malware_df['CPU'] == 'PowerPC']\n",
    "    \n",
    "    # Skip if not enough testing samples\n",
    "    if len(test_benign) < 5 or len(test_malware) < 5:\n",
    "        print(f\"  Skipping PowerPC (insufficient testing samples)\")\n",
    "        return None\n",
    "    \n",
    "    # Print data sizes\n",
    "    print(f\"  Training data: {len(train_benign)} benign, {len(train_malware)} malware\")\n",
    "    print(f\"  Testing data: {len(test_benign)} benign, {len(test_malware)} malware\")\n",
    "    \n",
    "    # Combine training data\n",
    "    train_data = pd.concat([train_benign, train_malware])\n",
    "    \n",
    "    # Combine testing data\n",
    "    test_data = pd.concat([test_benign, test_malware])\n",
    "    \n",
    "    # Prepare training data\n",
    "    X_train = train_data[feature_cols].fillna(0)\n",
    "    y_train = train_data['is_malware']\n",
    "    \n",
    "    # Prepare testing data\n",
    "    X_test = test_data[feature_cols].fillna(0)\n",
    "    y_test = test_data['is_malware']\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Results for multiple runs\n",
    "    run_results = []\n",
    "    \n",
    "    # Run with multiple random seeds\n",
    "    for seed in random_seeds:\n",
    "        print(f\"  Running with seed {seed}\")\n",
    "        \n",
    "        # Train Random Forest\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=seed)\n",
    "        rf.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = rf.predict(X_test_scaled)\n",
    "        y_scores = rf.predict_proba(X_test_scaled)[:, 1]  # Probability for class 1\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = calculate_metrics(y_test, y_pred, y_scores)\n",
    "        metrics['seed'] = seed\n",
    "        \n",
    "        # Add to results\n",
    "        run_results.append(metrics)\n",
    "    \n",
    "    # Cross-validation on training data\n",
    "    print(f\"  Performing {cv_folds}-fold cross-validation\")\n",
    "    cv_results = []\n",
    "    \n",
    "    # Use stratified k-fold to maintain class distribution\n",
    "    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_scaled, y_train)):\n",
    "        # Split data\n",
    "        X_train_fold = X_train_scaled[train_idx]\n",
    "        y_train_fold = y_train.iloc[train_idx]\n",
    "        X_val_fold = X_train_scaled[val_idx]\n",
    "        y_val_fold = y_train.iloc[val_idx]\n",
    "        \n",
    "        # Train model\n",
    "        rf_cv = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        rf_cv.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # Validate\n",
    "        y_val_pred = rf_cv.predict(X_val_fold)\n",
    "        y_val_scores = rf_cv.predict_proba(X_val_fold)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        fold_metrics = calculate_metrics(y_val_fold, y_val_pred, y_val_scores)\n",
    "        fold_metrics['fold'] = fold + 1\n",
    "        \n",
    "        # Add to results\n",
    "        cv_results.append(fold_metrics)\n",
    "    \n",
    "    # Calculate statistics across runs\n",
    "    f1_scores = [result['f1'] for result in run_results]\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    std_f1 = np.std(f1_scores)\n",
    "    cv_f1_scores = [result['f1'] for result in cv_results]\n",
    "    mean_cv_f1 = np.mean(cv_f1_scores)\n",
    "    std_cv_f1 = np.std(cv_f1_scores)\n",
    "    \n",
    "    # Confidence interval (95%)\n",
    "    ci_95 = stats.t.interval(0.95, len(f1_scores)-1, loc=mean_f1, scale=std_f1/np.sqrt(len(f1_scores)))\n",
    "    \n",
    "    print(f\"  Mean F1 Score: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "    print(f\"  95% Confidence Interval: [{ci_95[0]:.4f}, {ci_95[1]:.4f}]\")\n",
    "    print(f\"  Cross-Validation F1: {mean_cv_f1:.4f} ± {std_cv_f1:.4f}\")\n",
    "    \n",
    "    # Compile results\n",
    "    results = {\n",
    "        'train_cpu': train_cpu,\n",
    "        'train_samples': len(train_data),\n",
    "        'test_samples': len(test_data),\n",
    "        'mean_f1': mean_f1,\n",
    "        'std_f1': std_f1,\n",
    "        'ci_95_low': ci_95[0],\n",
    "        'ci_95_high': ci_95[1],\n",
    "        'mean_cv_f1': mean_cv_f1,\n",
    "        'std_cv_f1': std_cv_f1,\n",
    "        'run_results': run_results,\n",
    "        'cv_results': cv_results\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_f1_comparison(all_results, output_dir='./results'):\n",
    "    \"\"\"Plot and save F1 score comparison with error bars and custom colors\"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Extract data for plotting\n",
    "    train_cpus = [r['train_cpu'] for r in all_results]\n",
    "    mean_f1s = [r['mean_f1'] for r in all_results]\n",
    "    std_f1s = [r['std_f1'] for r in all_results]\n",
    "    cv_f1s = [r['mean_cv_f1'] for r in all_results]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Architecture': train_cpus,\n",
    "        'Mean F1 Score': mean_f1s,\n",
    "        'Std F1 Score': std_f1s,\n",
    "        'CV F1 Score': cv_f1s\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(f'{output_dir}/{timestamp}_powerpc_f1_comparison.csv', index=False)\n",
    "    \n",
    "    # Save detailed results to JSON\n",
    "    with open(f'{output_dir}/{timestamp}_detailed_results.json', 'w') as f:\n",
    "        # Convert NumPy values to Python types for JSON serialization\n",
    "        results_for_json = []\n",
    "        for result in all_results:\n",
    "            result_copy = result.copy()\n",
    "            for key, value in result_copy.items():\n",
    "                if isinstance(value, np.number):\n",
    "                    result_copy[key] = float(value)\n",
    "            results_for_json.append(result_copy)\n",
    "        \n",
    "        json.dump(results_for_json, f, indent=2)\n",
    "    \n",
    "    # Sort by mean F1 score\n",
    "    df = df.sort_values('Mean F1 Score', ascending=False)\n",
    "    \n",
    "    # Define colors matching the provided example image\n",
    "    # Deep purple, navy blue, teal, medium green, chartreuse\n",
    "    custom_colors = ['#2D004A', '#304887', '#3D7D7C', '#4CAA66', '#B9CF45']\n",
    "    \n",
    "    # Make sure we have enough colors\n",
    "    while len(custom_colors) < len(df):\n",
    "        custom_colors.extend(custom_colors)\n",
    "    \n",
    "    # --------- Main F1 Score Chart ---------\n",
    "    # Set the style for the plot\n",
    "    sns.set_style('whitegrid')\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Bar plot with error bars\n",
    "    bars = plt.bar(df['Architecture'], df['Mean F1 Score'], \n",
    "                  yerr=df['Std F1 Score'], capsize=5,\n",
    "                  color=custom_colors[:len(df)])\n",
    "    \n",
    "    # Add values on top of bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                 f'{height:.4f}±{df.iloc[i][\"Std F1 Score\"]:.4f}', \n",
    "                 ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Add CV F1 scores as line plot\n",
    "    plt.plot(df['Architecture'], df['CV F1 Score'], 'ro-', linewidth=2, \n",
    "             markersize=8, label='Cross-Validation F1')\n",
    "    \n",
    "    plt.title('F1 Scores on PowerPC by Training Architecture', fontsize=16)\n",
    "    plt.xlabel('Training Architecture', fontsize=14)\n",
    "    plt.ylabel('F1 Score', fontsize=14)\n",
    "    plt.ylim(0, 1.1)  # Set y-axis limit with some margin for the text\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot - both PNG and SVG formats\n",
    "    plt.savefig(f'{output_dir}/{timestamp}_powerpc_f1_comparison.png', dpi=300)\n",
    "    plt.savefig(f'{output_dir}/{timestamp}_powerpc_f1_comparison.svg', format='svg')\n",
    "    plt.close()\n",
    "    \n",
    "    # --------- Simple Colorful Chart (no error bars) ---------\n",
    "    # Create a simplified colorful chart as requested\n",
    "    sns.set_style('whitegrid')\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    # Create the bar chart with custom colors\n",
    "    simple_bars = plt.bar(\n",
    "        df['Architecture'], \n",
    "        df['Mean F1 Score'], \n",
    "        color=custom_colors[:len(df)]\n",
    "    )\n",
    "    \n",
    "    # Add the values on top of each bar\n",
    "    for bar in simple_bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width()/2., \n",
    "            height + 0.01,\n",
    "            f'{height:.4f}', \n",
    "            ha='center', \n",
    "            va='bottom', \n",
    "            fontsize=12,\n",
    "            fontweight='bold'\n",
    "        )\n",
    "    \n",
    "    # Set the labels and title\n",
    "    plt.title('F1 Scores on PowerPC by Training Architecture', fontsize=16)\n",
    "    plt.xlabel('Training Architecture', fontsize=14)\n",
    "    plt.ylabel('F1 Score', fontsize=14)\n",
    "    \n",
    "    # Set y-axis limits\n",
    "    plt.ylim(0, 1.1)\n",
    "    \n",
    "    # Add a light grid for better readability\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Save both PNG and SVG formats\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/{timestamp}_simple_f1_chart.png', dpi=300)\n",
    "    plt.savefig(f'{output_dir}/{timestamp}_simple_f1_chart.svg', format='svg')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create statistical significance heatmap\n",
    "    if len(all_results) > 1:\n",
    "        plot_significance_matrix(all_results, output_dir, timestamp)\n",
    "\n",
    "def plot_significance_matrix(all_results, output_dir, timestamp):\n",
    "    \"\"\"Plot a heatmap of statistical significance between pairs of architectures\"\"\"\n",
    "    n_archs = len(all_results)\n",
    "    architectures = [r['train_cpu'] for r in all_results]\n",
    "    \n",
    "    # Initialize p-value matrix\n",
    "    p_values = np.zeros((n_archs, n_archs))\n",
    "    \n",
    "    # Calculate p-values for all pairs\n",
    "    for i in range(n_archs):\n",
    "        for j in range(n_archs):\n",
    "            if i == j:\n",
    "                p_values[i, j] = 1.0  # Same architecture\n",
    "            else:\n",
    "                # Get F1 scores for both architectures\n",
    "                f1_scores_i = [r['f1'] for r in all_results[i]['run_results']]\n",
    "                f1_scores_j = [r['f1'] for r in all_results[j]['run_results']]\n",
    "                \n",
    "                # Perform t-test\n",
    "                _, p_value = stats.ttest_ind(f1_scores_i, f1_scores_j)\n",
    "                p_values[i, j] = p_value\n",
    "    \n",
    "    # Create significance mask (p < 0.05)\n",
    "    sig_mask = p_values < 0.05\n",
    "    \n",
    "    # Set the style\n",
    "    sns.set_style('white')\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Use -log10(p) for better visualization\n",
    "    log_p_values = -np.log10(p_values)\n",
    "    np.fill_diagonal(log_p_values, 0)  # Set diagonal to 0\n",
    "    \n",
    "    # Create heatmap with improved styling\n",
    "    sns.heatmap(log_p_values, annot=np.round(p_values, 3), \n",
    "                xticklabels=architectures, yticklabels=architectures,\n",
    "                cmap='viridis', mask=p_values >= 0.05,\n",
    "                linewidths=0.5, linecolor='white',\n",
    "                cbar_kws={'label': '-log10(p-value)'})\n",
    "    \n",
    "    # Add a red outline to statistically significant cells (p < 0.05)\n",
    "    for i in range(n_archs):\n",
    "        for j in range(n_archs):\n",
    "            if p_values[i, j] < 0.05 and i != j:\n",
    "                plt.gca().add_patch(plt.Rectangle((j, i), 1, 1, fill=False, edgecolor='red', lw=2))\n",
    "    \n",
    "    plt.title('Statistical Significance Between Architectures', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot - both PNG and SVG formats\n",
    "    plt.savefig(f'{output_dir}/{timestamp}_statistical_significance.png', dpi=300)\n",
    "    plt.savefig(f'{output_dir}/{timestamp}_statistical_significance.svg', format='svg')\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    # File paths\n",
    "    train_benign_file = '/home/tommy/cross-architecture/Experiment2/csv/20250228_cleanedtrain_benign_file_features.csv'\n",
    "    train_malware_file = '/home/tommy/cross-architecture/Experiment2/csv/20250228_cleanedtrain_malware_file_features.csv'\n",
    "    test_benign_file = '/home/tommy/cross-architecture/Experiment2/csv/20250228_cleanedtest_benign_file_features.csv'\n",
    "    test_malware_file = '/home/tommy/cross-architecture/Experiment2/csv/20250228_cleanedtest_malware_file_features.csv'\n",
    "    \n",
    "    # Load data\n",
    "    train_benign_df, train_malware_df, test_benign_df, test_malware_df = load_data(\n",
    "        train_benign_file, train_malware_file, test_benign_file, test_malware_file\n",
    "    )\n",
    "    \n",
    "    # Print CPU distributions\n",
    "    print(\"\\nTraining data CPU distribution:\")\n",
    "    train_cpu_dist = pd.DataFrame({\n",
    "        'Benign': train_benign_df['CPU'].value_counts(),\n",
    "        'Malware': train_malware_df['CPU'].value_counts()\n",
    "    }).fillna(0)\n",
    "    print(train_cpu_dist)\n",
    "    \n",
    "    print(\"\\nTesting data CPU distribution:\")\n",
    "    test_cpu_dist = pd.DataFrame({\n",
    "        'Benign': test_benign_df['CPU'].value_counts(),\n",
    "        'Malware': test_malware_df['CPU'].value_counts()\n",
    "    }).fillna(0)\n",
    "    print(test_cpu_dist)\n",
    "    \n",
    "    # Get features from all data\n",
    "    all_data = pd.concat([train_benign_df, train_malware_df, test_benign_df, test_malware_df])\n",
    "    feature_cols = get_features(all_data)\n",
    "    print(f\"\\nUsing {len(feature_cols)} non-zero features\")\n",
    "    \n",
    "    # Define CPU architectures for training\n",
    "    # Get all available architectures and add 'all'\n",
    "    available_cpus = sorted(list(set(train_benign_df['CPU'].unique()) | set(train_malware_df['CPU'].unique())))\n",
    "    available_cpus = [cpu for cpu in available_cpus if not pd.isna(cpu)]\n",
    "    train_cpus = available_cpus + ['all']\n",
    "    \n",
    "    # Experiment configuration\n",
    "    random_seeds = [42, 123, 456, 789, 101]  # Multiple seeds for reproducibility\n",
    "    cv_folds = 5  # Number of cross-validation folds\n",
    "    \n",
    "    # Evaluate each architecture on PowerPC\n",
    "    all_results = []\n",
    "    for train_cpu in train_cpus:\n",
    "        results = evaluate_on_powerpc(\n",
    "            train_cpu,\n",
    "            train_benign_df, train_malware_df,\n",
    "            test_benign_df, test_malware_df,\n",
    "            feature_cols,\n",
    "            random_seeds=random_seeds,\n",
    "            cv_folds=cv_folds\n",
    "        )\n",
    "        if results is not None:\n",
    "            all_results.append(results)\n",
    "    \n",
    "    # Plot and save results\n",
    "    if all_results:\n",
    "        plot_f1_comparison(all_results)\n",
    "    else:\n",
    "        print(\"No results to plot.\")\n",
    "    \n",
    "    print(\"\\nExperiment completed. Results saved to the 'results' directory.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved heatmap to ./output/cross_architecture_f1_matrix.svg\n",
      "Saved bar chart to ./output/powerpc_f1_comparison.svg\n",
      "All visualizations completed successfully!\n",
      "SVG files saved to the ./output directory\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "def create_f1_matrix_heatmap(output_file='cross_architecture_f1_matrix.svg'):\n",
    "    \"\"\"\n",
    "    Create a heatmap of the F1 scores for cross-architecture experiments.\n",
    "    \n",
    "    This function creates a heatmap similar to Image 1, showing F1 scores when\n",
    "    training on one architecture and testing on another.\n",
    "    \"\"\"\n",
    "    # Define the CPU architectures (in the desired order)\n",
    "    cpus = ['ARM', 'Advanced Micro Devices X86-64', 'Intel 80386', 'MIPS R3000']\n",
    "    \n",
    "    # Create a DataFrame to store the F1 scores\n",
    "    f1_scores = pd.DataFrame(\n",
    "        [\n",
    "            [1.0000, 0.6205, 0.8015, 0.5441],\n",
    "            [0.8889, 1.0000, 0.9817, 0.8391],\n",
    "            [0.8388, 0.9442, 1.0000, 0.7473],\n",
    "            [0.8660, 0.9244, 0.9674, 1.0000]\n",
    "        ],\n",
    "        index=cpus,\n",
    "        columns=cpus\n",
    "    )\n",
    "    \n",
    "    # Create the figure with appropriate size\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Create the heatmap\n",
    "    ax = sns.heatmap(\n",
    "        f1_scores, \n",
    "        annot=True, \n",
    "        fmt='.4f', \n",
    "        cmap='YlGnBu_r',  # Reversed YlGnBu colormap to match the example\n",
    "        vmin=0, \n",
    "        vmax=1,\n",
    "        square=True,\n",
    "        linewidths=1,\n",
    "        cbar_kws={'label': ''}\n",
    "    )\n",
    "    \n",
    "    # Set the labels and title\n",
    "    plt.title('Cross-Architecture F1 Score', fontsize=16, pad=20)\n",
    "    plt.xlabel('Test CPU', fontsize=14, labelpad=10)\n",
    "    plt.ylabel('Train CPU', fontsize=14, labelpad=10)\n",
    "    \n",
    "    # Adjust layout and save as SVG\n",
    "    plt.tight_layout()\n",
    "    os.makedirs('./output', exist_ok=True)\n",
    "    plt.savefig(f'./output/{output_file}', format='svg')\n",
    "    print(f\"Saved heatmap to ./output/{output_file}\")\n",
    "    \n",
    "    plt.close()\n",
    "\n",
    "def create_powerpc_comparison(output_file='powerpc_f1_comparison.svg'):\n",
    "    \"\"\"\n",
    "    Create a bar chart of F1 scores when training on different architectures\n",
    "    and testing on PowerPC.\n",
    "    \n",
    "    This function creates a bar chart similar to Image 2, showing F1 scores\n",
    "    for each training architecture when testing on PowerPC.\n",
    "    \"\"\"\n",
    "    # Define the data from the image\n",
    "    architectures = ['all', 'MIPS R3000', 'Intel 80386', 'Advanced Micro Devices X86-64', 'ARM']\n",
    "    f1_scores = [0.9535, 0.9361, 0.7460, 0.7401, 0.5849]\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'Architecture': architectures,\n",
    "        'F1 Score': f1_scores\n",
    "    })\n",
    "    \n",
    "    # Set the style for the plot\n",
    "    sns.set_style('whitegrid')\n",
    "    \n",
    "    # Create the figure\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    # Create the bar chart\n",
    "    bars = plt.bar(data['Architecture'], data['F1 Score'], color='steelblue')\n",
    "    \n",
    "    # Add the values on top of each bar\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width()/2., \n",
    "            height + 0.01,\n",
    "            f'{height:.4f}', \n",
    "            ha='center', \n",
    "            va='bottom', \n",
    "            fontsize=12\n",
    "        )\n",
    "    \n",
    "    # Set the labels and title\n",
    "    plt.title('F1 Scores on PowerPC by Training Architecture', fontsize=16)\n",
    "    plt.xlabel('Training Architecture', fontsize=14)\n",
    "    plt.ylabel('F1 Score', fontsize=14)\n",
    "    \n",
    "    # Set y-axis limits\n",
    "    plt.ylim(0, 1.1)\n",
    "    \n",
    "    # Create output directory and save as SVG\n",
    "    os.makedirs('./output', exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'./output/{output_file}', format='svg')\n",
    "    print(f\"Saved bar chart to ./output/{output_file}\")\n",
    "    \n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Generate both visualizations.\"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs('./output', exist_ok=True)\n",
    "    \n",
    "    # Generate the cross-architecture F1 matrix heatmap\n",
    "    create_f1_matrix_heatmap()\n",
    "    \n",
    "    # Generate the PowerPC comparison bar chart\n",
    "    create_powerpc_comparison()\n",
    "    \n",
    "    print(\"All visualizations completed successfully!\")\n",
    "    print(\"SVG files saved to the ./output directory\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Per-column arrays must each be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 134\u001b[0m\n\u001b[1;32m    130\u001b[0m     plt\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# Generate both color versions\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[43mcreate_powerpc_comparison\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     create_powerpc_comparison_alternativecolors()\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBar chart generation completed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 21\u001b[0m, in \u001b[0;36mcreate_powerpc_comparison\u001b[0;34m(output_file)\u001b[0m\n\u001b[1;32m     18\u001b[0m colors \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mcm\u001b[38;5;241m.\u001b[39mviridis(np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(architectures)))\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Create a DataFrame\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mArchitecture\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43marchitectures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mF1 Score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mf1_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mColor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolors\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Sort by F1 score to make visualization clearer\u001b[39;00m\n\u001b[1;32m     28\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF1 Score\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cross-architecture/lib/python3.12/site-packages/pandas/core/frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    774\u001b[0m     )\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/miniconda3/envs/cross-architecture/lib/python3.12/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cross-architecture/lib/python3.12/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/miniconda3/envs/cross-architecture/lib/python3.12/site-packages/pandas/core/internals/construction.py:664\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    662\u001b[0m         raw_lengths\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(val))\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m val\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 664\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_lengths:\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf using all scalar values, you must pass an index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Per-column arrays must each be 1-dimensional"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "def create_powerpc_comparison(output_file='powerpc_f1_comparison.svg'):\n",
    "    \"\"\"\n",
    "    Create a colorful bar chart of F1 scores when training on different architectures\n",
    "    and testing on PowerPC.\n",
    "    \"\"\"\n",
    "    # Define the data with simplified labels\n",
    "    architectures = ['all', 'MIPS R3000', 'Intel 80386', 'X86-64', 'ARM']\n",
    "    f1_scores = [0.9535, 0.9361, 0.7460, 0.7401, 0.5849]\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'Architecture': architectures,\n",
    "        'F1 Score': f1_scores\n",
    "    })\n",
    "    \n",
    "    # Sort by F1 score to make visualization clearer\n",
    "    data = data.sort_values('F1 Score', ascending=False)\n",
    "    \n",
    "    # Set the style for the plot\n",
    "    sns.set_style('whitegrid')\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    # Generate colors using a colormap\n",
    "    colormap = plt.cm.viridis\n",
    "    colors = [colormap(i) for i in np.linspace(0, 0.9, len(data))]\n",
    "    \n",
    "    # Create the bar chart with different colors for each bar\n",
    "    bars = plt.bar(\n",
    "        data['Architecture'], \n",
    "        data['F1 Score'], \n",
    "        color=colors\n",
    "    )\n",
    "    \n",
    "    # Add the values on top of each bar\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width()/2., \n",
    "            height + 0.01,\n",
    "            f'{height:.4f}', \n",
    "            ha='center', \n",
    "            va='bottom', \n",
    "            fontsize=12\n",
    "        )\n",
    "    \n",
    "    # Set the labels and title\n",
    "    plt.title('F1 Scores on PowerPC by Training Architecture', fontsize=16)\n",
    "    plt.xlabel('Training Architecture', fontsize=14)\n",
    "    plt.ylabel('F1 Score', fontsize=14)\n",
    "    \n",
    "    # Set y-axis limits\n",
    "    plt.ylim(0, 1.1)\n",
    "    \n",
    "    # Create output directory and save as SVG\n",
    "    os.makedirs('./output', exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'./output/{output_file}', format='svg')\n",
    "    print(f\"Saved colorful bar chart to ./output/{output_file}\")\n",
    "    \n",
    "    plt.close()\n",
    "\n",
    "def create_powerpc_comparison_brightcolors(output_file='powerpc_f1_comparison_bright.svg'):\n",
    "    \"\"\"\n",
    "    Create an alternative colorful bar chart with bright colors.\n",
    "    \"\"\"\n",
    "    # Define the data with simplified labels\n",
    "    architectures = ['all', 'MIPS R3000', 'Intel 80386', 'X86-64', 'ARM']\n",
    "    f1_scores = [0.9535, 0.9361, 0.7460, 0.7401, 0.5849]\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'Architecture': architectures,\n",
    "        'F1 Score': f1_scores\n",
    "    })\n",
    "    \n",
    "    # Sort by F1 score to make visualization clearer\n",
    "    data = data.sort_values('F1 Score', ascending=False)\n",
    "    \n",
    "    # Define bright colors - one for each bar\n",
    "    bright_colors = ['#FF9500', '#00B4D8', '#FF5C8D', '#4CAF50', '#9C27B0']\n",
    "    \n",
    "    # Set the style for the plot\n",
    "    sns.set_style('whitegrid')\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    # Create the bar chart with vibrant colors\n",
    "    bars = plt.bar(\n",
    "        data['Architecture'], \n",
    "        data['F1 Score'], \n",
    "        color=bright_colors[:len(data)]\n",
    "    )\n",
    "    \n",
    "    # Add the values on top of each bar\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width()/2., \n",
    "            height + 0.01,\n",
    "            f'{height:.4f}', \n",
    "            ha='center', \n",
    "            va='bottom', \n",
    "            fontsize=12,\n",
    "            fontweight='bold'\n",
    "        )\n",
    "    \n",
    "    # Set the labels and title\n",
    "    plt.title('F1 Scores on PowerPC by Training Architecture', fontsize=16)\n",
    "    plt.xlabel('Training Architecture', fontsize=14)\n",
    "    plt.ylabel('F1 Score', fontsize=14)\n",
    "    \n",
    "    # Set y-axis limits\n",
    "    plt.ylim(0, 1.1)\n",
    "    \n",
    "    # Add a light grid for better readability\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Create output directory and save as SVG\n",
    "    os.makedirs('./output', exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'./output/{output_file}', format='svg')\n",
    "    print(f\"Saved bright colorful bar chart to ./output/{output_file}\")\n",
    "    \n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate both color versions\n",
    "    create_powerpc_comparison()\n",
    "    create_powerpc_comparison_brightcolors()\n",
    "    print(\"Bar chart generation completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import os\n",
    "\n",
    "def load_data(train_malware_file, test_malware_file):\n",
    "    \"\"\"Load training and testing malware data\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    \n",
    "    # Load training data (only malware)\n",
    "    train_malware_df = pd.read_csv(train_malware_file)\n",
    "    \n",
    "    # Load testing data (only malware)\n",
    "    test_malware_df = pd.read_csv(test_malware_file)\n",
    "    \n",
    "    return train_malware_df, test_malware_df\n",
    "\n",
    "def get_features(df):\n",
    "    \"\"\"Get feature columns, excluding non-feature columns and zero columns\"\"\"\n",
    "    exclude_cols = ['file_name', 'CPU', 'label', 'family', 'is_malware']\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    # Filter out zero columns\n",
    "    non_zero_features = []\n",
    "    for col in feature_cols:\n",
    "        if df[col].sum() > 0:\n",
    "            non_zero_features.append(col)\n",
    "    \n",
    "    return non_zero_features\n",
    "\n",
    "def evaluate_family_classification(train_cpu, train_malware_df, test_malware_df, feature_cols):\n",
    "    \"\"\"Train on specified architecture and test on PowerPC for family classification\"\"\"\n",
    "    print(f\"Training on {train_cpu}, testing on PowerPC for family classification...\")\n",
    "    \n",
    "    # Get training data\n",
    "    if train_cpu == 'all':\n",
    "        # Use all training data\n",
    "        train_malware = train_malware_df\n",
    "    else:\n",
    "        # Filter by CPU\n",
    "        train_malware = train_malware_df[train_malware_df['CPU'] == train_cpu]\n",
    "    \n",
    "    # Skip if not enough training samples\n",
    "    if len(train_malware) < 10:\n",
    "        print(f\"  Skipping {train_cpu} (insufficient training samples)\")\n",
    "        return None\n",
    "    \n",
    "    # Get testing data for PowerPC\n",
    "    test_malware = test_malware_df[test_malware_df['CPU'] == 'PowerPC']\n",
    "    \n",
    "    # Skip if not enough testing samples\n",
    "    if len(test_malware) < 5:\n",
    "        print(f\"  Skipping PowerPC (insufficient testing samples)\")\n",
    "        return None\n",
    "    \n",
    "    # Print data sizes and family distributions\n",
    "    print(f\"  Training data: {len(train_malware)} malware samples\")\n",
    "    print(f\"  Testing data: {len(test_malware)} malware samples\")\n",
    "    \n",
    "    print(\"\\n  Training family distribution:\")\n",
    "    print(train_malware['family'].value_counts())\n",
    "    \n",
    "    print(\"\\n  Testing family distribution:\")\n",
    "    print(test_malware['family'].value_counts())\n",
    "    \n",
    "    # Encode family labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_malware['family_encoded'] = label_encoder.fit_transform(train_malware['family'])\n",
    "    # Apply the same transformation to test data\n",
    "    test_malware['family_encoded'] = label_encoder.transform(test_malware['family'])\n",
    "    \n",
    "    # Prepare training data\n",
    "    X_train = train_malware[feature_cols].fillna(0)\n",
    "    y_train = train_malware['family_encoded']\n",
    "    \n",
    "    # Prepare testing data\n",
    "    X_test = test_malware[feature_cols].fillna(0)\n",
    "    y_test = test_malware['family_encoded']\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = rf.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate F1 score (weighted for multi-class)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"\\n  Weighted F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Get more detailed performance metrics\n",
    "    report = classification_report(y_test, y_pred, \n",
    "                                  target_names=label_encoder.classes_,\n",
    "                                  output_dict=True)\n",
    "    \n",
    "    # Create a DataFrame from the report for easier manipulation\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    \n",
    "    # Print detailed report\n",
    "    print(\"\\n  Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "    \n",
    "    return f1, report_df, label_encoder.classes_\n",
    "\n",
    "def plot_f1_comparison(results, output_dir='./results'):\n",
    "    \"\"\"Plot and save F1 score comparison\"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get training CPUs and F1 scores\n",
    "    train_cpus = list(results.keys())\n",
    "    f1_scores = [results[cpu][0] for cpu in train_cpus]  # Extract F1 scores\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Architecture': train_cpus,\n",
    "        'F1 Score': f1_scores\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(f'{output_dir}/powerpc_family_f1_comparison.csv', index=False)\n",
    "    \n",
    "    # Sort by F1 score\n",
    "    df = df.sort_values('F1 Score', ascending=False)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    # Bar plot\n",
    "    bars = plt.bar(df['Architecture'], df['F1 Score'], color='steelblue')\n",
    "    \n",
    "    # Add values on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                 f'{height:.4f}', ha='center', va='bottom', fontsize=12)\n",
    "    \n",
    "    plt.title('Family Classification F1 Scores on PowerPC by Training Architecture', fontsize=16)\n",
    "    plt.xlabel('Training Architecture', fontsize=14)\n",
    "    plt.ylabel('Weighted F1 Score', fontsize=14)\n",
    "    plt.ylim(0, 1.1)  # Set y-axis limit with some margin for the text\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(f'{output_dir}/powerpc_family_f1_comparison.png', dpi=300)\n",
    "    print(f\"F1 score comparison plot saved to {output_dir}/powerpc_family_f1_comparison.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Also save per-family performance for the best model\n",
    "    best_arch = df.iloc[0]['Architecture']\n",
    "    best_report = results[best_arch][1]\n",
    "    best_families = results[best_arch][2]\n",
    "    \n",
    "    # Save detailed report to CSV\n",
    "    best_report.to_csv(f'{output_dir}/best_family_classification_report.csv')\n",
    "    \n",
    "    # Plot per-family F1 scores for the best model\n",
    "    plot_family_performance(best_report, best_families, best_arch, output_dir)\n",
    "\n",
    "def plot_family_performance(report_df, families, architecture, output_dir):\n",
    "    \"\"\"Plot per-family performance metrics for the best model\"\"\"\n",
    "    # Filter out the summary rows\n",
    "    family_report = report_df.loc[families]\n",
    "    \n",
    "    # Get F1 scores for each family\n",
    "    family_f1 = family_report['f1-score'].sort_values(ascending=False)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Bar plot\n",
    "    bars = plt.bar(family_f1.index, family_f1.values, color='lightseagreen')\n",
    "    \n",
    "    # Add values on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                 f'{height:.3f}', ha='center', va='bottom', fontsize=10, rotation=90)\n",
    "    \n",
    "    plt.title(f'Per-Family F1 Scores (Trained on {architecture})', fontsize=16)\n",
    "    plt.xlabel('Malware Family', fontsize=14)\n",
    "    plt.ylabel('F1 Score', fontsize=14)\n",
    "    plt.ylim(0, 1.1)  # Set y-axis limit with some margin for the text\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(f'{output_dir}/per_family_f1_scores.png', dpi=300)\n",
    "    print(f\"Per-family F1 scores plot saved to {output_dir}/per_family_f1_scores.png\")\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    # File paths\n",
    "    train_malware_file = '/home/tommy/cross-architecture/Experiment2/csv/20250228_cleanedtrain_malware_file_features.csv'\n",
    "    test_malware_file = '/home/tommy/cross-architecture/Experiment2/csv/20250228_cleanedtest_malware_file_features.csv'\n",
    "    \n",
    "    # Load data (only malware)\n",
    "    train_malware_df, test_malware_df = load_data(train_malware_file, test_malware_file)\n",
    "    \n",
    "    # Print CPU distributions\n",
    "    print(\"\\nTraining data CPU distribution:\")\n",
    "    train_cpu_dist = pd.DataFrame({\n",
    "        'Malware': train_malware_df['CPU'].value_counts()\n",
    "    }).fillna(0)\n",
    "    print(train_cpu_dist)\n",
    "    \n",
    "    print(\"\\nTesting data CPU distribution:\")\n",
    "    test_cpu_dist = pd.DataFrame({\n",
    "        'Malware': test_malware_df['CPU'].value_counts()\n",
    "    }).fillna(0)\n",
    "    print(test_cpu_dist)\n",
    "    \n",
    "    # Print family distributions\n",
    "    print(\"\\nTraining data family distribution:\")\n",
    "    print(train_malware_df['family'].value_counts())\n",
    "    \n",
    "    print(\"\\nTesting data family distribution:\")\n",
    "    print(test_malware_df['family'].value_counts())\n",
    "    \n",
    "    # Get features from all data\n",
    "    all_data = pd.concat([train_malware_df, test_malware_df])\n",
    "    feature_cols = get_features(all_data)\n",
    "    print(f\"\\nUsing {len(feature_cols)} non-zero features\")\n",
    "    \n",
    "    # Define CPU architectures for training\n",
    "    # Get all available architectures and add 'all'\n",
    "    available_cpus = sorted(list(set(train_malware_df['CPU'].unique())))\n",
    "    available_cpus = [cpu for cpu in available_cpus if not pd.isna(cpu)]\n",
    "    train_cpus = available_cpus + ['all']\n",
    "    \n",
    "    # Evaluate each architecture on PowerPC\n",
    "    results = {}\n",
    "    for train_cpu in train_cpus:\n",
    "        result = evaluate_family_classification(\n",
    "            train_cpu,\n",
    "            train_malware_df,\n",
    "            test_malware_df,\n",
    "            feature_cols\n",
    "        )\n",
    "        if result is not None:\n",
    "            results[train_cpu] = result\n",
    "    \n",
    "    # Plot and save results\n",
    "    if results:\n",
    "        plot_f1_comparison(results)\n",
    "    else:\n",
    "        print(\"No results to plot.\")\n",
    "    \n",
    "    print(\"\\nExperiment completed. Results saved to the 'results' directory.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cross-architecture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
